{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pandas as pd \n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Dense, LSTM, Dropout, TimeDistributed, Lambda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAR(tf.keras.models.Model):\n",
    "    def __init__(self, lstm_units, n_steps_in, n_steps_out, n_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True, return_state=True, input_shape=(n_steps_in, n_features))\n",
    "        self.dense_mu = tf.keras.layers.Dense(1) \n",
    "        self.dense_sigma = tf.keras.layers.Dense(1, activation='softplus')\n",
    "\n",
    "    def call(self, inputs, initial_state=None):\n",
    "        outputs, state_h, state_c = self.lstm(inputs, initial_state=initial_state)\n",
    "\n",
    "        mu = self.dense_mu(outputs)\n",
    "        sigma = self.dense_sigma(outputs)\n",
    "        state = [state_h, state_c]\n",
    "\n",
    "        return [mu, sigma, state]\n",
    "\n",
    "def log_gaussian_loss(mu, sigma, y_true):\n",
    "    \"\"\"\n",
    "    Gaussian loss function\n",
    "    \"\"\"\n",
    "    return -tf.reduce_sum(tfp.distributions.Normal(loc=mu, scale=sigma).log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/시계열_교안/3주차/Gemini_ETHUSD_d.csv',  skiprows=1, parse_dates=True, index_col='Date')\n",
    "df = df.sort_index().drop(['Symbol','Unix Timestamp'] , axis=1)\n",
    "\n",
    "test_cutoff_date = df.index.max() - timedelta(days=90)\n",
    "\n",
    "df_test = df[df.index > test_cutoff_date]['Close']\n",
    "df_train = df[df.index <= test_cutoff_date]['Close']\n",
    "\n",
    "\n",
    "############# 데이터를 정규화\n",
    "training_mean = df_train.mean() # train의 평균\n",
    "training_std = df_train.std() # train의 편차\n",
    "df_training_value = (df_train - training_mean) / training_std # 데이터 = (원본 - 평균) / 편차로 정규화\n",
    "\n",
    "test_mean = df_test.mean() # test의 평균\n",
    "test_std = df_test.std() # test의 편차\n",
    "df_test_value = (df_test - test_mean) / test_std # 데이터 = (원본 - 평균) / 편차로 정규화\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "\n",
    "raw_seq = df_training_value\n",
    "n_steps_in = 30\n",
    "n_steps_out = 1\n",
    "\n",
    "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
    "\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "y = y.reshape((y.shape[0], y.shape[1], n_features))\n",
    "\n",
    "test_raw_seq = df_test_value\n",
    "\n",
    "test_X, test_y = split_sequence(test_raw_seq, n_steps_in, n_steps_out)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_UNITS = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "model = DeepAR(LSTM_UNITS, n_steps_in, n_steps_out, n_features)\n",
    "\n",
    "# optmizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "# metric\n",
    "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        mu, sigma, _ = model(x)\n",
    "        loss = log_gaussian_loss(mu, sigma, y) # Forward Learning\n",
    "    # backword\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    rmse(y, mu)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_step(X, y)\n",
    "    print('Epoch %d, RMSE %.4f' % (epoch + 1, rmse.result().numpy()))\n",
    "    rmse.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플링된 값들을 예측값으로 반환.\n",
    "pred = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0].shape\n",
    "\n",
    "# median => 실제 예측값.\n",
    "# 5% 퍼센타일 => lower bound\n",
    "# 95% 퍼센타일 => upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape # 라벨의 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lower_bound = []\n",
    "upper_bound = []\n",
    "median = []\n",
    "for step_pred in pred[0]:\n",
    "                           # 정규화된 값을 원래대로.\n",
    "  lb = (np.quantile(step_pred, 0.05) + test_mean) * test_std\n",
    "  ub = (np.quantile(step_pred, 0.95) + test_mean) * test_std\n",
    "  med = (np.quantile(step_pred, 0.5) + test_mean) * test_std\n",
    "  lower_bound.append(lb)\n",
    "  upper_bound.append(ub)\n",
    "  median.append(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (12,6))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "\n",
    "df['Close'].plot(ax=ax)\n",
    "ax.vlines('2020-08-22', 0, 1000, linestyle='--', color='r', label='forecast boundary')\n",
    "ax.fill_between(df_test[30:].index, lower_bound, upper_bound, color='b', alpha=0.1, label='95% Confidence Interval')\n",
    "ax.plot(df_test[30:].index, median, label='Prediction')\n",
    "ax.legend(loc='upper left')\n",
    "# plt.suptitle(f\"ARIMA {optimal[0][0]} Prediction Result (r2 score: {r2}\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cee8b8822b488bb0e56ff9277324ceeb63e4c757e04f2ca7c9fc05709ba9211e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('tensorflow-2.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
